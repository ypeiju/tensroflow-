{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num=5\n",
    "epoch_num = 2\n",
    "batch_size = 3\n",
    "batch_total = int(sample_num/batch_size)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(sample_num=sample_num):\n",
    "    labels = np.asarray(range(0, sample_num))    \n",
    "    images = np.random.random([sample_num, 224, 224, 3])    \n",
    "    print('image size {},label size :{}'.format(images.shape, labels.shape))    \n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size (5, 224, 224, 3),label size :(5,)\n"
     ]
    }
   ],
   "source": [
    "def get_batch_data(batch_size=batch_size):    \n",
    "    images, label = generate_data()    # 数据类型转换为tf.float32    \n",
    "    images = tf.cast(images, tf.float32)   \n",
    "    label = tf.cast(label, tf.int32)     \n",
    "    #从tensor列表中按顺序或随机抽取一个tensor准备放入文件名称队列    \n",
    "    input_queue = tf.train.slice_input_producer([images, label], num_epochs=epoch_num, shuffle=False)     \n",
    "    #从文件名称队列中读取文件准备放入文件队列    \n",
    "    image_batch, label_batch = tf.train.batch(input_queue, batch_size=batch_size, num_threads=2, capacity=64, allow_smaller_final_batch=False)    \n",
    "    return image_batch, label_batch\n",
    "image_batch, label_batch = get_batch_data(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************\n",
      "(3, 224, 224, 3) [0 1 2]\n",
      "************\n",
      "(3, 224, 224, 3) [3 4 0]\n",
      "************\n",
      "(3, 224, 224, 3) [1 2 3]\n",
      "************\n",
      "done! now lets kill all the threads……\n",
      "all threads are asked to stop!\n",
      "all threads are stopped!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:     \n",
    "    # 先执行初始化工作    \n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    sess.run(tf.local_variables_initializer())     \n",
    "    # 开启一个协调器    \n",
    "    coord = tf.train.Coordinator()    \n",
    "    # 使用start_queue_runners 启动队列填充    \n",
    "    threads = tf.train.start_queue_runners(sess, coord)     \n",
    "    try:        \n",
    "        while not coord.should_stop():            \n",
    "            print('************')           \n",
    "            # 获取每一个batch中batch_size个样本和标签            \n",
    "            image_batch_v, label_batch_v = sess.run([image_batch, label_batch])            \n",
    "            print(image_batch_v.shape, label_batch_v)    \n",
    "    except tf.errors.OutOfRangeError:          #如果读取到文件队列末尾会抛出此异常        \n",
    "        print(\"done! now lets kill all the threads……\")    \n",
    "    finally:        \n",
    "        # 协调器coord发出所有线程终止信号        \n",
    "        coord.request_stop()        \n",
    "        print('all threads are asked to stop!')    \n",
    "        coord.join(threads) #把开启的线程加入主线程，等待threads结束    \n",
    "        print('all threads are stopped!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CpuTensorflow",
   "language": "python",
   "name": "cputf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
